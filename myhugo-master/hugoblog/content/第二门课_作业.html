+++
title= "正则化代码实现"
date= 2019-08-16T17:00:16+08:00
draft= false
author = "Angyi"
tags = ["神经网络"]
Menu = "ml"
summary= "在初始化过程中进行改进，来避免梯度爆炸和梯度消失问题。\n 在前向传播和反向传播中进行正则化，来避免过拟合问题。"
series =["ai"]
categories = ["机器学习&深度学习"]
toc = true
+++
<!DOCTYPE html><html><head>
      <title>translate_Regularization_answer</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css">
      
      

      
      
      
      
      
      

      <style>
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}

/* highlight */
pre[data-line] {
  position: relative;
  padding: 1em 0 1em 3em;
}
pre[data-line] .line-highlight-wrapper {
  position: absolute;
  top: 0;
  left: 0;
  background-color: transparent;
  display: block;
  width: 100%;
}

pre[data-line] .line-highlight {
  position: absolute;
  left: 0;
  right: 0;
  padding: inherit 0;
  margin-top: 1em;
  background: hsla(24, 20%, 50%,.08);
  background: linear-gradient(to right, hsla(24, 20%, 50%,.1) 70%, hsla(24, 20%, 50%,0));
  pointer-events: none;
  line-height: inherit;
  white-space: pre;
}

pre[data-line] .line-highlight:before, 
pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-start);
  position: absolute;
  top: .4em;
  left: .6em;
  min-width: 1em;
  padding: 0 .5em;
  background-color: hsla(24, 20%, 50%,.4);
  color: hsl(24, 20%, 95%);
  font: bold 65%/1.5 sans-serif;
  text-align: center;
  vertical-align: .3em;
  border-radius: 999px;
  text-shadow: none;
  box-shadow: 0 1px white;
}

pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-end);
  top: auto;
  bottom: .4em;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{padding:0 1.6em;margin-top:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li{margin-bottom:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{list-style-type:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  150px);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview  ">
      <h1 class="mume-header" id="regularization-%E6%AD%A3%E5%88%99%E5%8C%96">Regularization &#x6B63;&#x5219;&#x5316;</h1>

<h3 class="mume-header" id="%E4%B8%8A%E6%9C%9F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%87%BD%E6%95%B0%E7%9A%84%E6%80%9D%E8%B7%AF%E6%80%BB%E7%BB%93">&#x4E0A;&#x671F;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x6A21;&#x578B;&#x51FD;&#x6570;&#x7684;&#x601D;&#x8DEF;&#x603B;&#x7ED3;</h3>

<ol>
<li>Arguments-&#x8F93;&#x5165;:</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:center">&#x53C2;&#x6570;</th>
<th style="text-align:center">&#x542B;&#x4E49;</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">X</td>
<td style="text-align:center">input data, of shape (input size, number of examples)</td>
</tr>
<tr>
<td style="text-align:center">Y</td>
<td style="text-align:center">true &quot;label&quot; vector</td>
</tr>
<tr>
<td style="text-align:center">learning_rate</td>
<td style="text-align:center">learning rate of the optimization</td>
</tr>
<tr>
<td style="text-align:center">num_iterations</td>
<td style="text-align:center">number of iterations of the optimization loop</td>
</tr>
<tr>
<td style="text-align:center">print_cost</td>
<td style="text-align:center">If True, print the cost every 10000 iterations</td>
</tr>
<tr>
<td style="text-align:center">lambd</td>
<td style="text-align:center">regularization hyperparameter, scalar</td>
</tr>
<tr>
<td style="text-align:center">keep_prob</td>
<td style="text-align:center">probability of keeping a neuron active during drop-out, scalar.</td>
</tr>
</tbody>
</table>
<ol start="2">
<li>&#x603B;&#x7ED3;&#x4E00;&#x4E0B;&#xFF1A;model 5&#x6B65;&#xFF1A;</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:center">&#x5E8F;&#x53F7;</th>
<th style="text-align:center">&#x6B65;&#x9AA4;</th>
<th style="text-align:center">&#x8F93;&#x5165;</th>
<th style="text-align:center">&#x8F93;&#x51FA;</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">&#x521D;&#x59CB;&#x5316;parameters</td>
<td style="text-align:center">&#x8F93;&#x5165;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x7ED3;&#x6784;</td>
<td style="text-align:center">parameters</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">&#x524D;&#x5411;&#x4F20;&#x64AD;</td>
<td style="text-align:center">&#x8F93;&#x5165; x , parameters</td>
<td style="text-align:center">y^ cache&#x4E2D;&#x95F4;&#x91CF;</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">&#x8BA1;&#x7B97;&#x635F;&#x5931;&#x51FD;&#x6570;</td>
<td style="text-align:center">y^ y</td>
<td style="text-align:center">cost</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">&#x8BA1;&#x7B97;&#x68AF;&#x5EA6;</td>
<td style="text-align:center">X Y cache</td>
<td style="text-align:center">grads</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">&#x66F4;&#x65B0;&#x53C2;&#x6570;</td>
<td style="text-align:center">parameters grads</td>
<td style="text-align:center">parameters</td>
</tr>
</tbody>
</table>
<ol start="3">
<li>Returns-&#x8F93;&#x51FA;:<br>
parameters -- parameters learned by the model. They can then be used to predict.</li>
</ol>
<h2 class="mume-header" id="%E6%9C%AC%E6%9C%9F%E4%BB%BB%E5%8A%A1-%E6%AD%A3%E5%88%99%E5%8C%96%E5%AD%A6%E4%B9%A0">&#x672C;&#x671F;&#x4EFB;&#x52A1;--&#x6B63;&#x5219;&#x5316;&#x5B66;&#x4E60;</h2>

<h3 class="mume-header" id="%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87">&#x6570;&#x636E;&#x51C6;&#x5907;</h3>

<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token comment"># import packages</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">from</span> reg_utils <span class="token keyword">import</span> sigmoid<span class="token punctuation">,</span> relu<span class="token punctuation">,</span> plot_decision_boundary<span class="token punctuation">,</span> initialize_parameters<span class="token punctuation">,</span> load_2D_dataset<span class="token punctuation">,</span> predict_dec
<span class="token keyword">from</span> reg_utils <span class="token keyword">import</span> compute_cost<span class="token punctuation">,</span> predict<span class="token punctuation">,</span> forward_propagation<span class="token punctuation">,</span> backward_propagation<span class="token punctuation">,</span> update_parameters
<span class="token keyword">import</span> sklearn
<span class="token keyword">import</span> sklearn<span class="token punctuation">.</span>datasets
<span class="token keyword">import</span> scipy<span class="token punctuation">.</span>io
<span class="token keyword">from</span> testCases <span class="token keyword">import</span> <span class="token operator">*</span>

<span class="token keyword">import</span> warnings
warnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">&quot;ignore&quot;</span><span class="token punctuation">)</span>

<span class="token operator">%</span>matplotlib inline
plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">&apos;figure.figsize&apos;</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">7.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">)</span> <span class="token comment"># set default size of plots</span>
plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">&apos;image.interpolation&apos;</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">&apos;nearest&apos;</span>
<span class="token comment">#plt.rcParams[&apos;image.cmap&apos;] = &apos;gray&apos;</span>
</pre><pre data-role="codeBlock" data-info="python" class="language-python">train_X<span class="token punctuation">,</span> train_Y<span class="token punctuation">,</span> test_X<span class="token punctuation">,</span> test_Y <span class="token operator">=</span> load_2D_dataset<span class="token punctuation">(</span><span class="token punctuation">)</span>
</pre><p><img src="https://i.loli.net/2019/08/16/ZB1WobkgdKfTpNn.png" alt="output_7_0"><br>
<strong>&#x4F60;&#x7684;&#x76EE;&#x6807;</strong>&#xFF1A;&#x5C06;&#x56FE;&#x4E2D;&#x7684;&#x70B9;&#x5206;&#x7C7B;</p>
<h2 class="mume-header" id="1-non-regularized-model">1 - Non-regularized model</h2>

<p>You will use the following neural network (already implemented for you below). This model can be used:</p>
<ul>
<li>in <em>regularization mode</em> -- by setting the <code>lambd</code> input to a non-zero value. We use &quot;<code>lambd</code>&quot; instead of &quot;<code>lambda</code>&quot; because &quot;<code>lambda</code>&quot; is a reserved keyword in Python.</li>
<li>in <em>dropout mode</em> -- by setting the <code>keep_prob</code> to a value less than one</li>
</ul>
<p>&#x60A8;&#x5C06;&#x9996;&#x5148;&#x5C1D;&#x8BD5;&#x6CA1;&#x6709;&#x6B63;&#x5219;&#x5316;&#x7684;&#x6A21;&#x578B;&#xFF0C;&#x7136;&#x540E;&#x63A5;&#x7740;&#x5C1D;&#x8BD5;:</p>
<ul>
<li><em>L2 regularization</em> -- functions: &quot;<code>compute_cost_with_regularization()</code>&quot; and &quot;<code>backward_propagation_with_regularization()</code>&quot;</li>
<li><em>Dropout</em> -- functions: &quot;<code>forward_propagation_with_dropout()</code>&quot; and &quot;<code>backward_propagation_with_dropout()</code>&quot;</li>
</ul>
<p>In each part&#xFF0C;&#x60A8;&#x8981;&#x4F7F;&#x7528;&#x6B63;&#x786E;&#x7684;&#x8F93;&#x5165;&#x53BB;&#x8FD0;&#x884C;&#x6A21;&#x578B;&#xFF0C;&#x4EE5;&#x4FBF;&#x8C03;&#x7528;&#x60A8;&#x5DF2;&#x7ECF;&#x5B9E;&#x73B0;&#x7684;&#x529F;&#x80FD;&#x3002;&#x73B0;&#x5728;&#x8BA9;&#x6211;&#x4EEC;&#x6765;&#x719F;&#x6089;&#x4E0B;&#x6A21;&#x578B;</p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword">def</span> <span class="token function">model</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> learning_rate <span class="token operator">=</span> <span class="token number">0.3</span><span class="token punctuation">,</span> num_iterations <span class="token operator">=</span> <span class="token number">30000</span><span class="token punctuation">,</span> print_cost <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span> lambd <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> keep_prob <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

        
    grads <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    costs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>                            <span class="token comment"># to keep track of the cost</span>
    m <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>                        <span class="token comment"># number of examples</span>
    layers_dims <span class="token operator">=</span> <span class="token punctuation">[</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
    
    <span class="token comment"># Initialize parameters dictionary.</span>
    parameters <span class="token operator">=</span> initialize_parameters<span class="token punctuation">(</span>layers_dims<span class="token punctuation">)</span>

    <span class="token comment"># Loop (gradient descent)</span>

    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>

        <span class="token comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span>
        <span class="token keyword">if</span> keep_prob <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            a3<span class="token punctuation">,</span> cache <span class="token operator">=</span> forward_propagation<span class="token punctuation">(</span>X<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> keep_prob <span class="token operator">&lt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            a3<span class="token punctuation">,</span> cache <span class="token operator">=</span> forward_propagation_with_dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> parameters<span class="token punctuation">,</span> keep_prob<span class="token punctuation">)</span>
        
        <span class="token comment"># &#x635F;&#x5931;&#x51FD;&#x6570;&#x6DFB;&#x52A0;&#x6B63;&#x5219;&#x9879;</span>
        <span class="token keyword">if</span> lambd <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            cost <span class="token operator">=</span> compute_cost<span class="token punctuation">(</span>a3<span class="token punctuation">,</span> Y<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            cost <span class="token operator">=</span> compute_cost_with_regularization<span class="token punctuation">(</span>a3<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> parameters<span class="token punctuation">,</span> lambd<span class="token punctuation">)</span>
            
        <span class="token comment"># &#x5411;&#x540E;&#x4F20;&#x64AD;&#x8FC7;&#x7A0B;&#x6B63;&#x5219;&#x53D8;&#x5316;</span>
        <span class="token keyword">assert</span><span class="token punctuation">(</span>lambd<span class="token operator">==</span><span class="token number">0</span> <span class="token keyword">or</span> keep_prob<span class="token operator">==</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># it is possible to use both L2 regularization and dropout, </span>
                                            <span class="token comment"># but this assignment will only explore one at a time</span>
        <span class="token keyword">if</span> lambd <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> keep_prob <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            grads <span class="token operator">=</span> backward_propagation<span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> cache<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> lambd <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
            grads <span class="token operator">=</span> backward_propagation_with_regularization<span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> cache<span class="token punctuation">,</span> lambd<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> keep_prob <span class="token operator">&lt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            grads <span class="token operator">=</span> backward_propagation_with_dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> cache<span class="token punctuation">,</span> keep_prob<span class="token punctuation">)</span>
        
        <span class="token comment"># Update parameters.</span>
        parameters <span class="token operator">=</span> update_parameters<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span> grads<span class="token punctuation">,</span> learning_rate<span class="token punctuation">)</span>
        
        <span class="token comment"># Print the loss every 10000 iterations</span>
        <span class="token keyword">if</span> print_cost <span class="token keyword">and</span> i <span class="token operator">%</span> <span class="token number">10000</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Cost after iteration {}: {}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> cost<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> print_cost <span class="token keyword">and</span> i <span class="token operator">%</span> <span class="token number">1000</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            costs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cost<span class="token punctuation">)</span>
    
    <span class="token comment"># plot the cost</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>costs<span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">&apos;cost&apos;</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">&apos;iterations (x1,000)&apos;</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&quot;Learning rate =&quot;</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> parameters
</pre><p>&#x8BA9;&#x6211;&#x4EEC;&#x8BAD;&#x7EC3;&#x6CA1;&#x6709;&#x4EFB;&#x4F55;&#x6B63;&#x5219;&#x5316;&#x7684;&#x6A21;&#x578B;&#xFF0C;&#x5E76;&#x89C2;&#x5BDF;&#x8BAD;&#x7EC3;/&#x6D4B;&#x8BD5;&#x96C6;&#x7684;&#x51C6;&#x786E;&#x6027;&#x3002;</p>
<pre data-role="codeBlock" data-info="python" class="language-python">parameters <span class="token operator">=</span> model<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_Y<span class="token punctuation">)</span>
<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">&quot;On the training set:&quot;</span><span class="token punctuation">)</span>
predictions_train <span class="token operator">=</span> predict<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_Y<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>
<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">&quot;On the test set:&quot;</span><span class="token punctuation">)</span>
predictions_test <span class="token operator">=</span> predict<span class="token punctuation">(</span>test_X<span class="token punctuation">,</span> test_Y<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>
</pre><pre class="language-text">Cost after iteration 0: 0.6557412523481002
Cost after iteration 10000: 0.1632998752572419
Cost after iteration 20000: 0.13851642423239133
</pre>
<p><img src="https://i.loli.net/2019/08/16/nG7XYvpWaPJOAFg.png" alt="output_12_1"></p>
<pre class="language-text">On the training set:
Accuracy: 0.9478672985781991
On the test set:
Accuracy: 0.915
</pre>
<p>&#x8BAD;&#x7EC3;&#x96C6;&#x7684; Accuracy &#x662F; 94.8%&#xFF0C;&#x6D4B;&#x8BD5;&#x96C6;&#x7684; Accuracy &#x662F; 91.5%. This is the <strong>baseline model</strong> (you will observe the impact of regularization on this model). &#x8FD0;&#x884C;&#x4EE5;&#x4E0B;&#x4EE3;&#x7801;&#x6765;&#x7ED8;&#x5236;&#x6A21;&#x578B;&#x7684;&#x51B3;&#x7B56;&#x8FB9;&#x754C;</p>
<pre data-role="codeBlock" data-info="python" class="language-python">plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&quot;Model without regularization&quot;</span><span class="token punctuation">)</span>
axes <span class="token operator">=</span> plt<span class="token punctuation">.</span>gca<span class="token punctuation">(</span><span class="token punctuation">)</span>
axes<span class="token punctuation">.</span>set_xlim<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.75</span><span class="token punctuation">,</span><span class="token number">0.40</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
axes<span class="token punctuation">.</span>set_ylim<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.75</span><span class="token punctuation">,</span><span class="token number">0.65</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plot_decision_boundary<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> predict_dec<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span> x<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">,</span> train_X<span class="token punctuation">,</span> train_Y<span class="token punctuation">)</span>
</pre><p><img src="https://i.loli.net/2019/08/16/LzlHbqVhU9jnyx7.png" alt="output_14_0"><br>
<strong>&#x975E;&#x6B63;&#x5219;&#x5316;&#x6A21;&#x578B;&#x663E;&#x7136;&#x662F;&#x8FC7;&#x5EA6;&#x62DF;&#x5408;&#x8BAD;&#x7EC3;&#x96C6;&#xFF0C;&#x5B83;&#x62DF;&#x5408;&#x4E86;&#x566A;&#x97F3;&#x6570;&#x636E;&#xFF01;&#x73B0;&#x5728;&#x8BA9;&#x6211;&#x4EEC;&#x770B;&#x770B;&#x53EF;&#x4EE5;&#x51CF;&#x5C11;&#x8FC7;&#x5EA6;&#x62DF;&#x5408;&#x7684;&#x4E24;&#x79CD;&#x6280;&#x672F;&#x3002;</strong></p>
<h2 class="mume-header" id="2-l2-regularization">2 - L2 Regularization</h2>

<p>The standard way to avoid overfitting is called <strong>L2 regularization</strong>. It consists of appropriately modifying your cost function, from:<br>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>J</mi><mo>=</mo><mo>&#x2212;</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mstyle mathsize="1.2em"><mo stretchy="false">(</mo><mstyle mathsize="0.9em"><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mi>log</mi><mo>&#x2061;</mo><mrow><mo fence="true">(</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo fence="true">)</mo></mrow><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>&#x2212;</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi>log</mi><mo>&#x2061;</mo><mrow><mo fence="true">(</mo><mn>1</mn><mo>&#x2212;</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo fence="true">)</mo></mrow><mstyle mathsize="1.2em"><mo stretchy="false">)</mo></mstyle></mstyle></mstyle></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} \tag{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.929066em;vertical-align:-1.277669em;"></span><span class="mord">&#x2212;</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">m</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">&#x2211;</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord sizing reset-size6 size7"><span class="mopen">(</span></span><span class="mord sizing reset-size6 size5"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9129999999999999em;"><span style="top:-3.013em;margin-right:0.05555555555555556em;"><span class="pstrut" style="height:2.6em;"></span><span class="sizing reset-size5 size2 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop sizing reset-size6 size5">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner sizing reset-size6 size5"><span class="mopen sizing reset-size5 size6 delimcenter" style="top:0.024999999999999994em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9129999999999999em;"><span style="top:-3.013em;margin-right:0.05555555555555556em;"><span class="pstrut" style="height:2.6em;"></span><span class="sizing reset-size5 size2 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mathdefault mtight">L</span><span class="mclose mtight">]</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose sizing reset-size5 size6 delimcenter" style="top:0.024999999999999994em;"><span class="delimsizing size1">)</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin sizing reset-size6 size5">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8999999999999999em;vertical-align:-0.22499999999999998em;"></span><span class="mopen sizing reset-size6 size5">(</span><span class="mord sizing reset-size6 size5">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin sizing reset-size6 size5">&#x2212;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.27251em;vertical-align:-0.37251000000000006em;"></span><span class="mord sizing reset-size6 size5"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9129999999999999em;"><span style="top:-3.013em;margin-right:0.05555555555555556em;"><span class="pstrut" style="height:2.6em;"></span><span class="sizing reset-size5 size2 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose sizing reset-size6 size5">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop sizing reset-size6 size5">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner sizing reset-size6 size5"><span class="mopen sizing reset-size5 size6 delimcenter" style="top:0.024999999999999994em;"><span class="delimsizing size1">(</span></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x2212;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9129999999999999em;"><span style="top:-3.013em;margin-right:0.05555555555555556em;"><span class="pstrut" style="height:2.6em;"></span><span class="sizing reset-size5 size2 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mathdefault mtight">L</span><span class="mclose mtight">]</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose sizing reset-size5 size6 delimcenter" style="top:0.024999999999999994em;"><span class="delimsizing size1">)</span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord sizing reset-size6 size7"><span class="mclose">)</span></span></span><span class="tag"><span class="strut" style="height:2.929066em;vertical-align:-1.277669em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span><br>
To:<br>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>J</mi><mrow><mi>r</mi><mi>e</mi><mi>g</mi><mi>u</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>z</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mstyle mathsize="0.9em"><munder><munder><mrow><mo>&#x2212;</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mstyle mathsize="1.2em"><mo stretchy="false">(</mo><mstyle mathsize="0.9em"><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mi>log</mi><mo>&#x2061;</mo><mrow><mo fence="true">(</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo fence="true">)</mo></mrow><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>&#x2212;</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi>log</mi><mo>&#x2061;</mo><mrow><mo fence="true">(</mo><mn>1</mn><mo>&#x2212;</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo fence="true">)</mo></mrow><mstyle mathsize="1.2em"><mo stretchy="false">)</mo></mstyle></mstyle></mstyle></mrow><mo stretchy="true">&#x23DF;</mo></munder><mtext>cross-entropy&#xA0;cost</mtext></munder><mo>+</mo><munder><munder><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac><mfrac><mi>&#x3BB;</mi><mn>2</mn></mfrac><munder><mo>&#x2211;</mo><mi>l</mi></munder><munder><mo>&#x2211;</mo><mi>k</mi></munder><munder><mo>&#x2211;</mo><mi>j</mi></munder><msubsup><mi>W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>j</mi></mrow><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo><mn>2</mn></mrow></msubsup></mrow><mo stretchy="true">&#x23DF;</mo></munder><mtext>L2&#xA0;regularization&#xA0;cost</mtext></munder></mstyle></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost} \tag{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.04398em;">z</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.8214234em;vertical-align:-2.3739165000000004em;"></span><span class="mord munder sizing reset-size6 size5"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6083409999999996em;"><span style="top:-1.1002826666666667em;"><span class="pstrut" style="height:3.6083410000000002em;"></span><span class="sizing reset-size5 size2 mtight"><span class="mord text mtight"><span class="mord mtight">cross-entropy&#xA0;cost</span></span></span></span><span style="top:-3.608341em;"><span class="pstrut" style="height:3.6083410000000002em;"></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.608341em;"><span class="svg-align" style="top:-1.710336em;"><span class="pstrut" style="height:3.6083410000000002em;"></span><span class="stretchy" style="height:0.548em;min-width:1.6em;"><span class="brace-left" style="height:0.548em;"><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMinYMin slice"><path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"/></svg></span><span class="brace-center" style="height:0.548em;"><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMidYMin slice"><path d="M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"/></svg></span><span class="brace-right" style="height:0.548em;"><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMaxYMin slice"><path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"/></svg></span></span></span><span style="top:-3.6083410000000002em;"><span class="pstrut" style="height:3.6083410000000002em;"></span><span class="mord"><span class="mord">&#x2212;</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.256996em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size5"><span class="mord"><span class="mord mathdefault">m</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size5"><span class="mord"><span class="mord">1</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6083410000000002em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size2 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">&#x2211;</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size2 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord sizing reset-size6 size7"><span class="mopen">(</span></span><span class="mord sizing reset-size6 size5"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9129999999999999em;"><span style="top:-3.013em;margin-right:0.05555555555555556em;"><span class="pstrut" style="height:2.6em;"></span><span class="sizing reset-size5 size2 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop sizing reset-size6 size5">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner sizing reset-size6 size5"><span class="mopen sizing reset-size5 size6 delimcenter" style="top:0.024999999999999994em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9129999999999999em;"><span style="top:-3.013em;margin-right:0.05555555555555556em;"><span class="pstrut" style="height:2.6em;"></span><span class="sizing reset-size5 size2 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mathdefault mtight">L</span><span class="mclose mtight">]</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose sizing reset-size5 size6 delimcenter" style="top:0.024999999999999994em;"><span class="delimsizing size1">)</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin sizing reset-size6 size5">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mopen sizing reset-size6 size5">(</span><span class="mord sizing reset-size6 size5">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin sizing reset-size6 size5">&#x2212;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord sizing reset-size6 size5"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9129999999999999em;"><span style="top:-3.013em;margin-right:0.05555555555555556em;"><span class="pstrut" style="height:2.6em;"></span><span class="sizing reset-size5 size2 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose sizing reset-size6 size5">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop sizing reset-size6 size5">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner sizing reset-size6 size5"><span class="mopen sizing reset-size5 size6 delimcenter" style="top:0.024999999999999994em;"><span class="delimsizing size1">(</span></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x2212;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9129999999999999em;"><span style="top:-3.013em;margin-right:0.05555555555555556em;"><span class="pstrut" style="height:2.6em;"></span><span class="sizing reset-size5 size2 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mathdefault mtight">L</span><span class="mclose mtight">]</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose sizing reset-size5 size6 delimcenter" style="top:0.024999999999999994em;"><span class="delimsizing size1">)</span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord sizing reset-size6 size7"><span class="mclose">)</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:1.8980050000000002em;"><span></span></span></span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:2.6376850000000003em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin sizing reset-size6 size5">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:3.6983265000000003em;vertical-align:-2.5265301em;"></span><span class="mord munder sizing reset-size6 size5"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.301996em;"><span style="top:-0.6243669999999998em;"><span class="pstrut" style="height:3.301996em;"></span><span class="sizing reset-size5 size2 mtight"><span class="mord text mtight"><span class="mord mtight">L2&#xA0;regularization&#xA0;cost</span></span></span></span><span style="top:-3.301996em;"><span class="pstrut" style="height:3.301996em;"></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.301996em;"><span class="svg-align" style="top:-1.2873269999999999em;"><span class="pstrut" style="height:3.301996em;"></span><span class="stretchy" style="height:0.548em;min-width:1.6em;"><span class="brace-left" style="height:0.548em;"><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMinYMin slice"><path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"/></svg></span><span class="brace-center" style="height:0.548em;"><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMidYMin slice"><path d="M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"/></svg></span><span class="brace-right" style="height:0.548em;"><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMaxYMin slice"><path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"/></svg></span></span></span><span style="top:-3.301996em;"><span class="pstrut" style="height:3.301996em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.256996em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size5"><span class="mord"><span class="mord mathdefault">m</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size5"><span class="mord"><span class="mord">1</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.301996em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size5"><span class="mord"><span class="mord">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size5"><span class="mord"><span class="mord mathdefault">&#x3BB;</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size2 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">&#x2211;</span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size2 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">&#x2211;</span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em;"><span style="top:-1.899995em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size2 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">&#x2211;</span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:1.366669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9447999999999999em;"><span style="top:-2.3681360000000002em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.6em;"></span><span class="sizing reset-size6 size2 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.0947999999999998em;margin-right:0.05em;"><span class="pstrut" style="height:2.6em;"></span><span class="sizing reset-size6 size2 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mclose mtight">]</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.34852799999999995em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:2.014669em;"><span></span></span></span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:2.807255666666667em;"><span></span></span></span></span></span></span><span class="tag"><span class="strut" style="height:3.9740369999999996em;vertical-align:-2.5265301em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>&#x8BA9;&#x6211;&#x4EEC;&#x4FEE;&#x6539;&#x4E0B;&#x635F;&#x5931;&#x51FD;&#x6570;&#x5E76;&#x89C2;&#x5BDF;&#x7ED3;&#x679C;.</p>
<p><strong>Exercise</strong>: Implement <code>compute_cost_with_regularization()</code> which computes the cost given by formula (2). To calculate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><munder><mo>&#x2211;</mo><mi>k</mi></munder><munder><mo>&#x2211;</mo><mi>j</mi></munder><msubsup><mi>W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>j</mi></mrow><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo><mn>2</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">\sum\limits_k\sum\limits_j W_{k,j}^{[l]2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.158577em;vertical-align:-1.113777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7500049999999999em;"><span style="top:-2.097887em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.000005em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop op-symbol small-op">&#x2211;</span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:1.002113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7500050000000001em;"><span style="top:-2.122331em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.0000050000000003em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop op-symbol small-op">&#x2211;</span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:1.113777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.3986920000000005em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mclose mtight">]</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.4374159999999999em;"><span></span></span></span></span></span></span></span></span></span>  , use :</p>
<pre data-role="codeBlock" data-info="python" class="language-python">np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>square<span class="token punctuation">(</span>Wl<span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><p>Note that you have to do this for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">W^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">W^{[2]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">2</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>3</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">W^{[3]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">3</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span>, then sum the three terms and multiply by <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac><mfrac><mi>&#x3BB;</mi><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{m} \frac{\lambda}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2251079999999999em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">&#x3BB;</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>.</p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token comment"># GRADED FUNCTION: compute_cost_with_regularization</span>

<span class="token keyword">def</span> <span class="token function">compute_cost_with_regularization</span><span class="token punctuation">(</span>A3<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> parameters<span class="token punctuation">,</span> lambd<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Implement the cost function with L2 regularization. See formula (2) above.
    
    Arguments:
    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)
    Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)
    parameters -- python dictionary containing parameters of the model
    
    Returns:
    cost - value of the regularized loss function (formula (2))
    &quot;&quot;&quot;</span>
    m <span class="token operator">=</span> Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    W1 <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">&quot;W1&quot;</span><span class="token punctuation">]</span>
    W2 <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">&quot;W2&quot;</span><span class="token punctuation">]</span>
    W3 <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">&quot;W3&quot;</span><span class="token punctuation">]</span>
    
    cross_entropy_cost <span class="token operator">=</span> compute_cost<span class="token punctuation">(</span>A3<span class="token punctuation">,</span> Y<span class="token punctuation">)</span> <span class="token comment"># This gives you the cross-entropy part of the cost</span>
    
     <span class="token comment"># &#x6B63;&#x5219;&#x5316;&#x635F;&#x5931;&#x51FD;&#x6570;</span>
    L2_regularization_cost <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">/</span>m <span class="token operator">*</span> lambd<span class="token operator">/</span><span class="token number">2</span> <span class="token operator">*</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>square<span class="token punctuation">(</span>W1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">+</span>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>square<span class="token punctuation">(</span>W2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">+</span>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>square<span class="token punctuation">(</span>W3<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    
    cost <span class="token operator">=</span> cross_entropy_cost <span class="token operator">+</span> L2_regularization_cost
    
    <span class="token keyword">return</span> cost
</pre><pre data-role="codeBlock" data-info="python" class="language-python">A3<span class="token punctuation">,</span> Y_assess<span class="token punctuation">,</span> parameters <span class="token operator">=</span> compute_cost_with_regularization_test_case<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;cost = &quot;</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>compute_cost_with_regularization<span class="token punctuation">(</span>A3<span class="token punctuation">,</span> Y_assess<span class="token punctuation">,</span> parameters<span class="token punctuation">,</span> lambd <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><pre class="language-text">cost = 1.78648594516
</pre>
<p>&#x5F53;&#x7136;&#xFF0C;&#x56E0;&#x4E3A;&#x4F60;&#x6539;&#x53D8;&#x4E86;&#x6210;&#x672C;&#xFF0C;&#x4F60;&#x4E5F;&#x5FC5;&#x987B;&#x6539;&#x53D8;&#x540E;&#x5411;&#x4F20;&#x64AD;&#xFF01;&#x6240;&#x6709;&#x7684;&#x68AF;&#x5EA6;&#x90FD;&#x5FC5;&#x987B;&#x8BA1;&#x7B97;&#x8FD9;&#x4E2A;&#x65B0;&#x7684;&#x6210;&#x672C;&#x3002;</p>
<p><strong>Exercise</strong>: Implement the changes needed in backward propagation to take into account regularization. The changes only concern dW1, dW2 and dW3. For each, you have to add the regularization term&apos;s gradient (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>W</mi></mrow></mfrac><mo stretchy="false">(</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mfrac><mi>&#x3BB;</mi><mi>m</mi></mfrac><msup><mi>W</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>&#x3BB;</mi><mi>m</mi></mfrac><mi>W</mi></mrow><annotation encoding="application/x-tex">\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2251079999999999em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">&#x3BB;</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2251079999999999em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">&#x3BB;</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>).</p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token comment"># GRADED FUNCTION: backward_propagation_with_regularization</span>

<span class="token keyword">def</span> <span class="token function">backward_propagation_with_regularization</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> cache<span class="token punctuation">,</span> lambd<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Implements the backward propagation of our baseline model to which we added an L2 regularization.
    
    Arguments:
    X -- input dataset, of shape (input size, number of examples)
    Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)
    cache -- cache output from forward_propagation()
    lambd -- regularization hyperparameter, scalar
    
    Returns:
    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables
    &quot;&quot;&quot;</span>
    
    m <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    <span class="token punctuation">(</span>Z1<span class="token punctuation">,</span> A1<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> Z2<span class="token punctuation">,</span> A2<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> Z3<span class="token punctuation">,</span> A3<span class="token punctuation">,</span> W3<span class="token punctuation">,</span> b3<span class="token punctuation">)</span> <span class="token operator">=</span> cache
    
    dZ3 <span class="token operator">=</span> A3 <span class="token operator">-</span> Y
    
    <span class="token comment">### START CODE HERE ### (approx. 1 line)</span>
    dW3 <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span>m <span class="token operator">*</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>dZ3<span class="token punctuation">,</span> A2<span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">+</span>  lambd<span class="token operator">/</span>m <span class="token operator">*</span> W3
    <span class="token comment">### END CODE HERE ###</span>
    db3 <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span>m <span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dZ3<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    
    dA2 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W3<span class="token punctuation">.</span>T<span class="token punctuation">,</span> dZ3<span class="token punctuation">)</span>
    dZ2 <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>dA2<span class="token punctuation">,</span> np<span class="token punctuation">.</span>int64<span class="token punctuation">(</span>A2 <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment">### START CODE HERE ### (approx. 1 line)</span>
    dW2 <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span>m <span class="token operator">*</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>dZ2<span class="token punctuation">,</span> A1<span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">+</span> lambd<span class="token operator">/</span>m <span class="token operator">*</span> W2
    <span class="token comment">### END CODE HERE ###</span>
    db2 <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span>m <span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dZ2<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    
    dA1 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W2<span class="token punctuation">.</span>T<span class="token punctuation">,</span> dZ2<span class="token punctuation">)</span>
    dZ1 <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>dA1<span class="token punctuation">,</span> np<span class="token punctuation">.</span>int64<span class="token punctuation">(</span>A1 <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment">### START CODE HERE ### (approx. 1 line)</span>
    dW1 <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span>m <span class="token operator">*</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>dZ1<span class="token punctuation">,</span> X<span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">+</span> lambd<span class="token operator">/</span>m <span class="token operator">*</span> W1
    <span class="token comment">### END CODE HERE ###</span>
    db1 <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span>m <span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dZ1<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    
    gradients <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&quot;dZ3&quot;</span><span class="token punctuation">:</span> dZ3<span class="token punctuation">,</span> <span class="token string">&quot;dW3&quot;</span><span class="token punctuation">:</span> dW3<span class="token punctuation">,</span> <span class="token string">&quot;db3&quot;</span><span class="token punctuation">:</span> db3<span class="token punctuation">,</span><span class="token string">&quot;dA2&quot;</span><span class="token punctuation">:</span> dA2<span class="token punctuation">,</span>
                 <span class="token string">&quot;dZ2&quot;</span><span class="token punctuation">:</span> dZ2<span class="token punctuation">,</span> <span class="token string">&quot;dW2&quot;</span><span class="token punctuation">:</span> dW2<span class="token punctuation">,</span> <span class="token string">&quot;db2&quot;</span><span class="token punctuation">:</span> db2<span class="token punctuation">,</span> <span class="token string">&quot;dA1&quot;</span><span class="token punctuation">:</span> dA1<span class="token punctuation">,</span> 
                 <span class="token string">&quot;dZ1&quot;</span><span class="token punctuation">:</span> dZ1<span class="token punctuation">,</span> <span class="token string">&quot;dW1&quot;</span><span class="token punctuation">:</span> dW1<span class="token punctuation">,</span> <span class="token string">&quot;db1&quot;</span><span class="token punctuation">:</span> db1<span class="token punctuation">}</span>
    
    <span class="token keyword">return</span> gradients
</pre><pre data-role="codeBlock" data-info="python" class="language-python">X_assess<span class="token punctuation">,</span> Y_assess<span class="token punctuation">,</span> cache <span class="token operator">=</span> backward_propagation_with_regularization_test_case<span class="token punctuation">(</span><span class="token punctuation">)</span>

grads <span class="token operator">=</span> backward_propagation_with_regularization<span class="token punctuation">(</span>X_assess<span class="token punctuation">,</span> Y_assess<span class="token punctuation">,</span> cache<span class="token punctuation">,</span> lambd <span class="token operator">=</span> <span class="token number">0.7</span><span class="token punctuation">)</span>
<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">&quot;dW1 = &quot;</span><span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token string">&quot;dW1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">&quot;dW2 = &quot;</span><span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token string">&quot;dW2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">&quot;dW3 = &quot;</span><span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token string">&quot;dW3&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><pre class="language-text">dW1 = [[-0.25604646  0.12298827 -0.28297129]
 [-0.17706303  0.34536094 -0.4410571 ]]
dW2 = [[ 0.79276486  0.85133918]
 [-0.0957219  -0.01720463]
 [-0.13100772 -0.03750433]]
dW3 = [[-1.77691347 -0.11832879 -0.09397446]]
</pre>
<p>Let&apos;s now run the model with L2 regularization <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><mi>&#x3BB;</mi><mo>=</mo><mn>0.7</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\lambda = 0.7)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">&#x3BB;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">7</span><span class="mclose">)</span></span></span></span>. The <code>model()</code> function will call:</p>
<ul>
<li><code>compute_cost_with_regularization</code> instead of <code>compute_cost</code></li>
<li><code>backward_propagation_with_regularization</code> instead of <code>backward_propagation</code></li>
</ul>
<pre data-role="codeBlock" data-info="python" class="language-python">parameters <span class="token operator">=</span> model<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_Y<span class="token punctuation">,</span> lambd <span class="token operator">=</span> <span class="token number">0.7</span><span class="token punctuation">)</span>
<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">&quot;On the train set:&quot;</span><span class="token punctuation">)</span>
predictions_train <span class="token operator">=</span> predict<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_Y<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>
<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">&quot;On the test set:&quot;</span><span class="token punctuation">)</span>
predictions_test <span class="token operator">=</span> predict<span class="token punctuation">(</span>test_X<span class="token punctuation">,</span> test_Y<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>
</pre><pre class="language-text">Cost after iteration 0: 0.6974484493131264
Cost after iteration 10000: 0.26849188732822393
Cost after iteration 20000: 0.2680916337127301
</pre>
<p><img src="https://i.loli.net/2019/08/16/tFRfBvKnow5HWqY.png" alt="output_23_1"><br>
On the train set:<br>
Accuracy: 0.938388625592<br>
On the test set:<br>
Accuracy: 0.93</p>
<p>&#x606D;&#x559C;&#xFF0C;&#x6D4B;&#x8BD5;&#x96C6;&#x7CBE;&#x5EA6;&#x63D0;&#x9AD8;&#x5230;&#x4E86;93&#xFF05;&#x3002;</p>
<p>&#x4F60;&#x4E0D;&#x518D;&#x8FC7;&#x5EA6;&#x62DF;&#x5408;&#x8BAD;&#x7EC3;&#x96C6;&#x6570;&#x636E;&#xFF0C;&#x8BA9;&#x6211;&#x4EEC;&#x7ED8;&#x5236;&#x51B3;&#x7B56;&#x8FB9;&#x754C;.</p>
<pre data-role="codeBlock" data-info="python" class="language-python">plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&quot;Model with L2-regularization&quot;</span><span class="token punctuation">)</span>
axes <span class="token operator">=</span> plt<span class="token punctuation">.</span>gca<span class="token punctuation">(</span><span class="token punctuation">)</span>
axes<span class="token punctuation">.</span>set_xlim<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.75</span><span class="token punctuation">,</span><span class="token number">0.40</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
axes<span class="token punctuation">.</span>set_ylim<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.75</span><span class="token punctuation">,</span><span class="token number">0.65</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plot_decision_boundary<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> predict_dec<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span> x<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">,</span> train_X<span class="token punctuation">,</span> train_Y<span class="token punctuation">)</span>
</pre><p><img src="https://i.loli.net/2019/08/16/RUSithb1G9lYE3c.png" alt="output_25_0"><br>
<strong>Observations</strong>:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>&#x3BB;</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">&#x3BB;</span></span></span></span> &#x662F;&#x4E00;&#x4E2A;&#x4F60;&#x53EF;&#x4EE5;&#x7528;&#x5F00;&#x53D1;&#x96C6;&#x53BB;&#x8C03;&#x6574;&#x7684;&#x8D85;&#x53C2;&#x6570;.</li>
<li>L2 &#x6B63;&#x5219;&#x5316;&#x4F7F;&#x5F97;&#x4F60;&#x7684;&#x51B3;&#x7B56;&#x8FB9;&#x754C;&#x66F4;&#x52A0;&#x5E73;&#x6ED1;. &#x5982;&#x679C; <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>&#x3BB;</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">&#x3BB;</span></span></span></span> &#x592A;&#x5927;, &#x4E5F;&#x6709;&#x53EF;&#x80FD;&#x201C;&#x8FC7;&#x5EA6;&#x5E73;&#x6ED1;&#x201D;&#xFF0C;&#x5BFC;&#x81F4;&#x9AD8;&#x504F;&#x5DEE;&#x6A21;&#x578B;&#x3002;</li>
</ul>
<p><strong>&#x4F60;&#x5E94;&#x8BE5;&#x8BB0;&#x4F4F;&#x4EC0;&#x4E48;</strong> -- the implications of L2-regularization on:</p>
<ul>
<li>The cost computation:
<ul>
<li>A regularization term is added to the cost</li>
</ul>
</li>
<li>The backpropagation function:
<ul>
<li>There are extra terms in the gradients with respect to weight matrices</li>
</ul>
</li>
<li>Weights end up smaller (&quot;weight decay&quot;):
<ul>
<li>Weights are pushed to smaller values.</li>
</ul>
</li>
</ul>
<h2 class="mume-header" id="3-dropout">3 - Dropout</h2>

<p>&#x6700;&#x540E;, <strong>dropout&#xFF08;&#x968F;&#x673A;&#x5931;&#x6D3B;&#xFF09;</strong> &#x662F;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x6240;&#x7279;&#x6709;&#x7684;&#x88AB;&#x5E7F;&#x6CDB;&#x4F7F;&#x7528;&#x7684;&#x6B63;&#x5219;&#x5316;&#x6280;&#x672F;&#x3002;<strong>&#x5B83;&#x5728;&#x6BCF;&#x6B21;&#x8FED;&#x4EE3;&#x4E2D;&#x968F;&#x673A;&#x5173;&#x95ED;&#x4E00;&#x4E9B;&#x795E;&#x7ECF;&#x5143;</strong>&#x3002;</p>
<p>&#x5F53;&#x4F60;&#x5173;&#x95ED;&#x4E00;&#x4E9B;&#x795E;&#x7ECF;&#x5143;&#x65F6;&#xFF0C;&#x4F60;&#x5B9E;&#x9645;&#x4E0A;&#x4FEE;&#x6539;&#x4E86;&#x4F60;&#x7684;&#x6A21;&#x578B;&#x3002;&#x76F8;&#x5F53;&#x4E8E;&#x5728;&#x6BCF;&#x6B21;&#x8FED;&#x4EE3;&#x4E2D;&#xFF0C;&#x4F60;&#x90FD;&#x8BAD;&#x7EC3;&#x4E86;&#x4E00;&#x4E2A;&#x4E0D;&#x540C;&#x7684;&#x6A21;&#x578B;&#xFF0C;&#x4F60;&#x53EA;&#x4F7F;&#x7528;&#x4F60;&#x7684;&#x795E;&#x7ECF;&#x5143;&#x7684;&#x4E00;&#x4E2A;&#x5B50;&#x96C6;&#x3002;&#x968F;&#x673A;&#x5931;&#x6D3B;&#x4F7F;&#x6BCF;&#x4E00;&#x4E2A;&#x795E;&#x7ECF;&#x5143;&#x4E0D;&#x518D;&#x4F9D;&#x8D56;&#x53E6;&#x4E00;&#x4E2A;&#x7279;&#x5B9A;&#x7684;&#x795E;&#x7ECF;&#x5143;&#xFF0C;&#x56E0;&#x4E3A;&#x5176;&#x4ED6;&#x7684;&#x795E;&#x7ECF;&#x5143;&#x968F;&#x65F6;&#x53EF;&#x80FD;&#x88AB;&#x5173;&#x95ED;&#x3002;&#x968F;&#x673A;&#x5931;&#x6D3B;&#x80FD;&#x9632;&#x6B62;&#x8FC7;&#x62DF;&#x5408;&#x3002;</p>
<h3 class="mume-header" id="31-forward-propagation-with-dropout">3.1 - Forward propagation with dropout</h3>

<p><strong>Instructions</strong>:<br>
&#x5982;&#x679C;&#x4F60;&#x60F3;&#x5173;&#x95ED;&#x7B2C;&#x4E00;&#x5C42;&#x548C;&#x7B2C;&#x4E8C;&#x5C42;&#x4E2D;&#x7684;&#x67D0;&#x4E9B;&#x795E;&#x7ECF;&#x5143;. &#x4F60;&#x9700;&#x8981;&#x6267;&#x884C;&#x4E0B;&#x9762;&#x7684;&#x56DB;&#x4E2A;&#x6B65;&#x9AA4;:</p>
<ol>
<li>In lecture, we dicussed creating a variable <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>d</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">d^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span> with the same shape as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span> using <code>np.random.rand()</code> to randomly get numbers between 0 and 1. Here, you will use a vectorized implementation, so create a random matrix <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>D</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mo stretchy="false">[</mo><msup><mi>d</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><msup><mi>d</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msup><mi>d</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">m</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span> of the same dimension as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">A^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span>.</li>
<li>Set each entry of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>D</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">D^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span> to be 0 with probability (<code>1-keep_prob</code>) or 1 with probability (<code>keep_prob</code>), by thresholding values in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>D</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">D^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span> appropriately. Hint: to set all the entries of a matrix X to 0 (if entry is less than 0.5) or 1 (if entry is more than 0.5) you would do: <code>X = (X &lt; 0.5)</code>. Note that 0 and 1 are respectively equivalent to False and True.</li>
<li>Set <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">A^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span> to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>&#x2217;</mo><msup><mi>D</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">A^{[1]} * D^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x2217;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span>. (You are shutting down some neurons). You can think of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>D</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">D^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span> as a mask, so that when it is multiplied with another matrix, it shuts down some of the values.</li>
<li>Divide <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">A^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span> by <code>keep_prob</code>. By doing this you are assuring that the result of the cost will still have the same expected value as without drop-out. (This technique is also called inverted dropout.)</li>
</ol>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token comment"># GRADED FUNCTION: forward_propagation_with_dropout</span>

<span class="token keyword">def</span> <span class="token function">forward_propagation_with_dropout</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> parameters<span class="token punctuation">,</span> keep_prob <span class="token operator">=</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.
    
    Arguments:
    X -- input dataset, of shape (2, number of examples)
    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;, &quot;W3&quot;, &quot;b3&quot;:
                    W1 -- weight matrix of shape (20, 2)
                    b1 -- bias vector of shape (20, 1)
                    W2 -- weight matrix of shape (3, 20)
                    b2 -- bias vector of shape (3, 1)
                    W3 -- weight matrix of shape (1, 3)
                    b3 -- bias vector of shape (1, 1)
    keep_prob - probability of keeping a neuron active during drop-out, scalar
    
    Returns:
    A3 -- last activation value, output of the forward propagation, of shape (1,1)
    cache -- tuple, information stored for computing the backward propagation
    &quot;&quot;&quot;</span>
    
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    
    <span class="token comment"># retrieve parameters</span>
    W1 <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">&quot;W1&quot;</span><span class="token punctuation">]</span>
    b1 <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">&quot;b1&quot;</span><span class="token punctuation">]</span>
    W2 <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">&quot;W2&quot;</span><span class="token punctuation">]</span>
    b2 <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">&quot;b2&quot;</span><span class="token punctuation">]</span>
    W3 <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">&quot;W3&quot;</span><span class="token punctuation">]</span>
    b3 <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">&quot;b3&quot;</span><span class="token punctuation">]</span>
    
    <span class="token comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span>
    Z1 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W1<span class="token punctuation">,</span> X<span class="token punctuation">)</span> <span class="token operator">+</span> b1
    A1 <span class="token operator">=</span> relu<span class="token punctuation">(</span>Z1<span class="token punctuation">)</span>
    <span class="token comment">### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span>
    D1 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>A1<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> A1<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>     <span class="token comment"># Step 1: initialize matrix D1 = np.random.rand(..., ...)</span>
    D1 <span class="token operator">=</span> D1 <span class="token operator">&lt;</span> keep_prob                               <span class="token comment"># Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span>
    A1 <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>D1<span class="token punctuation">,</span> A1<span class="token punctuation">)</span>                          <span class="token comment"># Step 3: shut down some neurons of A1</span>
    A1 <span class="token operator">=</span> A1 <span class="token operator">/</span> keep_prob                               <span class="token comment"># Step 4: scale the value of neurons that haven&apos;t been shut down</span>
    <span class="token comment">### END CODE HERE ###</span>
    Z2 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W2<span class="token punctuation">,</span> A1<span class="token punctuation">)</span> <span class="token operator">+</span> b2
    A2 <span class="token operator">=</span> relu<span class="token punctuation">(</span>Z2<span class="token punctuation">)</span>
    <span class="token comment">### START CODE HERE ### (approx. 4 lines)</span>
    D2 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>A2<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> A2<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>     <span class="token comment"># Step 1: initialize matrix D2 = np.random.rand(..., ...)</span>
    D2 <span class="token operator">=</span> D2 <span class="token operator">&lt;</span> keep_prob                               <span class="token comment"># Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)</span>
    A2 <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>D2<span class="token punctuation">,</span> A2<span class="token punctuation">)</span>                          <span class="token comment"># Step 3: shut down some neurons of A2</span>
    A2 <span class="token operator">=</span> A2 <span class="token operator">/</span> keep_prob                               <span class="token comment"># Step 4: scale the value of neurons that haven&apos;t been shut down</span>
    <span class="token comment">### END CODE HERE ###</span>
    Z3 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W3<span class="token punctuation">,</span> A2<span class="token punctuation">)</span> <span class="token operator">+</span> b3
    A3 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>Z3<span class="token punctuation">)</span>
    
    cache <span class="token operator">=</span> <span class="token punctuation">(</span>Z1<span class="token punctuation">,</span> D1<span class="token punctuation">,</span> A1<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> Z2<span class="token punctuation">,</span> D2<span class="token punctuation">,</span> A2<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> Z3<span class="token punctuation">,</span> A3<span class="token punctuation">,</span> W3<span class="token punctuation">,</span> b3<span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> A3<span class="token punctuation">,</span> cache
</pre><pre data-role="codeBlock" data-info="python" class="language-python">X_assess<span class="token punctuation">,</span> parameters <span class="token operator">=</span> forward_propagation_with_dropout_test_case<span class="token punctuation">(</span><span class="token punctuation">)</span>

A3<span class="token punctuation">,</span> cache <span class="token operator">=</span> forward_propagation_with_dropout<span class="token punctuation">(</span>X_assess<span class="token punctuation">,</span> parameters<span class="token punctuation">,</span> keep_prob <span class="token operator">=</span> <span class="token number">0.7</span><span class="token punctuation">)</span>
<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">&quot;A3 = &quot;</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>A3<span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><pre class="language-text">A3 = [[ 0.36974721  0.00305176  0.04565099  0.49683389  0.36974721]]
</pre>
<h3 class="mume-header" id="32-backward-propagation-with-dropout">3.2 - Backward propagation with dropout</h3>

<p><strong>Instruction</strong>:<br>
&#x62E5;&#x6709;&#x968F;&#x673A;&#x5931;&#x6D3B;&#x7684;&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x5176;&#x5B9E;&#x5F88;&#x7B80;&#x5355;. &#x4F60;&#x53EA;&#x9700;&#x6267;&#x884C;&#x4E0B;&#x9762;&#x4E24;&#x4E2A;&#x6B65;&#x9AA4;:</p>
<ol>
<li>You had previously shut down some neurons during forward propagation, by applying a mask <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>D</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">D^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span> to <code>A1</code>. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>D</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">D^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span> to <code>dA1</code>.</li>
<li>During forward propagation, you had divided <code>A1</code> by <code>keep_prob</code>. In backpropagation, you&apos;ll therefore have to divide <code>dA1</code> by <code>keep_prob</code> again (the calculus interpretation is that if <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">A^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span> is scaled by <code>keep_prob</code>, then its derivative <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">dA^{[1]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord mathdefault">d</span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span> is also scaled by the same <code>keep_prob</code>).</li>
</ol>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token comment"># GRADED FUNCTION: backward_propagation_with_dropout</span>

<span class="token keyword">def</span> <span class="token function">backward_propagation_with_dropout</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> cache<span class="token punctuation">,</span> keep_prob<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Implements the backward propagation of our baseline model to which we added dropout.
    
    Arguments:
    X -- input dataset, of shape (2, number of examples)
    Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)
    cache -- cache output from forward_propagation_with_dropout()
    keep_prob - probability of keeping a neuron active during drop-out, scalar
    
    Returns:
    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables
    &quot;&quot;&quot;</span>
    
    m <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    <span class="token punctuation">(</span>Z1<span class="token punctuation">,</span> D1<span class="token punctuation">,</span> A1<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> Z2<span class="token punctuation">,</span> D2<span class="token punctuation">,</span> A2<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> Z3<span class="token punctuation">,</span> A3<span class="token punctuation">,</span> W3<span class="token punctuation">,</span> b3<span class="token punctuation">)</span> <span class="token operator">=</span> cache
    
    dZ3 <span class="token operator">=</span> A3 <span class="token operator">-</span> Y
    dW3 <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span>m <span class="token operator">*</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>dZ3<span class="token punctuation">,</span> A2<span class="token punctuation">.</span>T<span class="token punctuation">)</span>
    db3 <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span>m <span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dZ3<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    dA2 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W3<span class="token punctuation">.</span>T<span class="token punctuation">,</span> dZ3<span class="token punctuation">)</span>
    <span class="token comment">### START CODE HERE ### (&#x2248; 2 lines of code)</span>
    dA2 <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>dA2<span class="token punctuation">,</span> D2<span class="token punctuation">)</span>      <span class="token comment"># Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span>
    dA2 <span class="token operator">=</span> dA2 <span class="token operator">/</span> keep_prob           <span class="token comment"># Step 2: Scale the value of neurons that haven&apos;t been shut down</span>
    <span class="token comment">### END CODE HERE ###</span>
    dZ2 <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>dA2<span class="token punctuation">,</span> np<span class="token punctuation">.</span>int64<span class="token punctuation">(</span>A2 <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    dW2 <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span>m <span class="token operator">*</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>dZ2<span class="token punctuation">,</span> A1<span class="token punctuation">.</span>T<span class="token punctuation">)</span>
    db2 <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span>m <span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dZ2<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    
    dA1 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W2<span class="token punctuation">.</span>T<span class="token punctuation">,</span> dZ2<span class="token punctuation">)</span>
    <span class="token comment">### START CODE HERE ### (&#x2248; 2 lines of code)</span>
    dA1 <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>dA1<span class="token punctuation">,</span> D1<span class="token punctuation">)</span>      <span class="token comment"># Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span>
    dA1 <span class="token operator">=</span> dA1 <span class="token operator">/</span> keep_prob           <span class="token comment"># Step 2: Scale the value of neurons that haven&apos;t been shut down</span>
    <span class="token comment">### END CODE HERE ###</span>
    dZ1 <span class="token operator">=</span> np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>dA1<span class="token punctuation">,</span> np<span class="token punctuation">.</span>int64<span class="token punctuation">(</span>A1 <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    dW1 <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span>m <span class="token operator">*</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>dZ1<span class="token punctuation">,</span> X<span class="token punctuation">.</span>T<span class="token punctuation">)</span>
    db1 <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">/</span>m <span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dZ1<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    
    gradients <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&quot;dZ3&quot;</span><span class="token punctuation">:</span> dZ3<span class="token punctuation">,</span> <span class="token string">&quot;dW3&quot;</span><span class="token punctuation">:</span> dW3<span class="token punctuation">,</span> <span class="token string">&quot;db3&quot;</span><span class="token punctuation">:</span> db3<span class="token punctuation">,</span><span class="token string">&quot;dA2&quot;</span><span class="token punctuation">:</span> dA2<span class="token punctuation">,</span>
                 <span class="token string">&quot;dZ2&quot;</span><span class="token punctuation">:</span> dZ2<span class="token punctuation">,</span> <span class="token string">&quot;dW2&quot;</span><span class="token punctuation">:</span> dW2<span class="token punctuation">,</span> <span class="token string">&quot;db2&quot;</span><span class="token punctuation">:</span> db2<span class="token punctuation">,</span> <span class="token string">&quot;dA1&quot;</span><span class="token punctuation">:</span> dA1<span class="token punctuation">,</span> 
                 <span class="token string">&quot;dZ1&quot;</span><span class="token punctuation">:</span> dZ1<span class="token punctuation">,</span> <span class="token string">&quot;dW1&quot;</span><span class="token punctuation">:</span> dW1<span class="token punctuation">,</span> <span class="token string">&quot;db1&quot;</span><span class="token punctuation">:</span> db1<span class="token punctuation">}</span>
    
    <span class="token keyword">return</span> gradients
</pre><pre data-role="codeBlock" data-info="python" class="language-python">X_assess<span class="token punctuation">,</span> Y_assess<span class="token punctuation">,</span> cache <span class="token operator">=</span> backward_propagation_with_dropout_test_case<span class="token punctuation">(</span><span class="token punctuation">)</span>

gradients <span class="token operator">=</span> backward_propagation_with_dropout<span class="token punctuation">(</span>X_assess<span class="token punctuation">,</span> Y_assess<span class="token punctuation">,</span> cache<span class="token punctuation">,</span> keep_prob <span class="token operator">=</span> <span class="token number">0.8</span><span class="token punctuation">)</span>

<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">&quot;dA1 = &quot;</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>gradients<span class="token punctuation">[</span><span class="token string">&quot;dA1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">&quot;dA2 = &quot;</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>gradients<span class="token punctuation">[</span><span class="token string">&quot;dA2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><pre class="language-text">dA1 = [[ 0.36544439  0.         -0.00188233  0.         -0.17408748]
 [ 0.65515713  0.         -0.00337459  0.         -0.        ]]
dA2 = [[ 0.58180856  0.         -0.00299679  0.         -0.27715731]
 [ 0.          0.53159854 -0.          0.53159854 -0.34089673]
 [ 0.          0.         -0.00292733  0.         -0.        ]]
</pre>
<p>&#x73B0;&#x5728;&#x8BA9;&#x6211;&#x4EEC;&#x7528;dropout&#xFF08;keep_prob = 0.86&#xFF09;&#x6765;&#x8FD0;&#x884C;&#x6A21;&#x578B;&#x3002;&#x8FD9;&#x610F;&#x5473;&#x7740;&#x5728;&#x6BCF;&#x4E00;&#x6B21;&#x8FED;&#x4EE3;&#x4E2D;&#xFF0C;&#x90FD;&#x4EE5;24&#xFF05;&#x7684;&#x6982;&#x7387;&#x5173;&#x95ED;&#x7B2C;1&#x5C42;&#x548C;&#x7B2C;2&#x5C42;&#x7684;&#x6BCF;&#x4E2A;&#x795E;&#x7ECF;&#x5143;&#x3002;The function <code>model()</code> will now call:</p>
<ul>
<li><code>forward_propagation_with_dropout</code> instead of <code>forward_propagation</code>.</li>
<li><code>backward_propagation_with_dropout</code> instead of <code>backward_propagation</code>.</li>
</ul>
<pre data-role="codeBlock" data-info="python" class="language-python">parameters <span class="token operator">=</span> model<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_Y<span class="token punctuation">,</span> keep_prob <span class="token operator">=</span> <span class="token number">0.86</span><span class="token punctuation">,</span> learning_rate <span class="token operator">=</span> <span class="token number">0.3</span><span class="token punctuation">)</span>

<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">&quot;On the train set:&quot;</span><span class="token punctuation">)</span>
predictions_train <span class="token operator">=</span> predict<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span> train_Y<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>
<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">&quot;On the test set:&quot;</span><span class="token punctuation">)</span>
predictions_test <span class="token operator">=</span> predict<span class="token punctuation">(</span>test_X<span class="token punctuation">,</span> test_Y<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>
</pre><pre class="language-text">Cost after iteration 0: 0.6543912405149825
Cost after iteration 10000: 0.0610169865749056
Cost after iteration 20000: 0.060582435798513114
</pre>
<p><img src="https://i.loli.net/2019/08/16/WLn5xrCtmHlKo63.png" alt="output_34_1"><br>
On the train set:<br>
Accuracy: 0.928909952607<br>
On the test set:<br>
Accuracy: 0.95</p>
<p>Dropout &#x8868;&#x73B0;&#x7684;&#x5F88;&#x4F18;&#x5F02;&#xFF01;&#x6D4B;&#x8BD5;&#x96C6; accuracy &#x518D;&#x6B21;&#x63D0;&#x9AD8;&#xFF08;&#x8FBE;&#x5230;95&#xFF05;&#xFF09;&#xFF01;&#x4F60;&#x7684;&#x6A21;&#x578B;&#x5BF9;&#x8BAD;&#x7EC3;&#x96C6;&#x4E0D;&#x518D;&#x8FC7;&#x62DF;&#x5408;&#xFF0C;&#x5E76;&#x4E14;&#x5728;&#x6D4B;&#x8BD5;&#x96C6;&#x4E0A;&#x8868;&#x73B0;&#x4E5F;&#x5F88;&#x597D;&#x3002;&#x6CD5;&#x56FD;&#x8DB3;&#x7403;&#x961F;&#x5C06;&#x6C38;&#x8FDC;&#x611F;&#x6FC0;&#x4F60;&#xFF01;</p>
<p>&#x8FD0;&#x884C;&#x4E0B;&#x9762;&#x7684;&#x4EE3;&#x7801;&#x6765;&#x7ED8;&#x5236;&#x51B3;&#x7B56;&#x8FB9;&#x754C;</p>
<pre data-role="codeBlock" data-info="python" class="language-python">plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&quot;Model with dropout&quot;</span><span class="token punctuation">)</span>
axes <span class="token operator">=</span> plt<span class="token punctuation">.</span>gca<span class="token punctuation">(</span><span class="token punctuation">)</span>
axes<span class="token punctuation">.</span>set_xlim<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.75</span><span class="token punctuation">,</span><span class="token number">0.40</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
axes<span class="token punctuation">.</span>set_ylim<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.75</span><span class="token punctuation">,</span><span class="token number">0.65</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plot_decision_boundary<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> predict_dec<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span> x<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">,</span> train_X<span class="token punctuation">,</span> train_Y<span class="token punctuation">)</span>
</pre><p><img src="https://i.loli.net/2019/08/16/CUiRJeQx1sFMyX5.png" alt="output_36_0"></p>
<p><strong>Note</strong>:</p>
<ul>
<li>&#x4F7F;&#x7528;dropout&#x7684;&#x4E00;&#x4E2A;&#x5E38;&#x89C1;&#x9519;&#x8BEF;&#x5C31;&#x662F;&#x5728;&#x8BAD;&#x7EC3;&#x548C;&#x6D4B;&#x8BD5;&#x65F6;&#x90FD;&#x4F7F;&#x7528;&#x5B83;&#x3002;&#x4F60;&#x53EA;&#x9700;&#x5728;&#x8BAD;&#x7EC3;&#x96C6;&#x4E2D;&#x4F7F;&#x7528;&#x5B83;</li>
<li>Deep learning frameworks like <a href="https://www.tensorflow.org/api_docs/python/tf/nn/dropout">tensorflow</a>, <a href="http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html">PaddlePaddle</a>, <a href="https://keras.io/layers/core/#dropout">keras</a> or <a href="http://caffe.berkeleyvision.org/tutorial/layers/dropout.html">caffe</a> come with a dropout layer implementation. &#x4E0D;&#x8981;&#x7D27;&#x5F20;&#xFF0C;&#x4F60;&#x5F88;&#x5FEB;&#x5C31;&#x4F1A;&#x5B66;&#x5230;&#x8FD9;&#x4E9B;&#x6846;&#x67B6;</li>
</ul>
<p><strong>&#x5173;&#x4E8E;dropout&#x4F60;&#x9700;&#x8981;&#x8BB0;&#x4F4F;&#x7684;&#x662F;:</strong></p>
<ul>
<li>Dropout &#x662F;&#x4E00;&#x79CD;&#x6B63;&#x5219;&#x5316;&#x6280;&#x672F;.</li>
<li>&#x4F60;&#x53EA;&#x9700;&#x5728;&#x8BAD;&#x7EC3;&#x671F;&#x95F4;&#x4F7F;&#x7528;&#x5B83;&#xFF0C;&#x6D4B;&#x8BD5;&#x671F;&#x95F4;&#x4E0D;&#x8981;&#x4F7F;&#x7528;.</li>
<li>&#x524D;&#x5411;&#x548C;&#x540E;&#x5411;&#x4F20;&#x64AD;&#x90FD;&#x4F1A;&#x4F7F;&#x7528;&#x5230;dropout.</li>
<li>During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.</li>
</ul>
<h2 class="mume-header" id="4-conclusions">4 - Conclusions</h2>

<p><strong>&#x4EE5;&#x4E0B;&#x662F;&#x4E09;&#x4E2A;&#x6A21;&#x578B;&#x7684;&#x5BF9;&#x6BD4;&#x7ED3;&#x679C;&#xFF1A;</strong>:</p>
<table> 
    <tbody><tr>
        <td>
        model 
        </td>
        <td>
        train accuracy
        </td>
        <td>
        test accuracy
        </td>
    </tr>    
    <tr>
        <td>
        3-layer NN without regularization
        </td>
        <td>
        95%
        </td>
        <td>
        91.5%
        </td>
    </tr>
    <tr>
        <td>
        3-layer NN with L2-regularization
        </td>
        <td>
        94%
        </td>
        <td>
        93%
        </td>
    </tr>
    <tr>
        <td>
        3-layer NN with dropout
        </td>
        <td>
        93%
        </td>
        <td>
        95%
        </td>
    </tr>
</tbody></table> 
<p>&#x8BF7;&#x6CE8;&#x610F;&#xFF0C;&#x6B63;&#x5219;&#x5316;&#x4F1A;&#x6709;&#x635F;&#x4F60;&#x8BAD;&#x7EC3;&#x96C6;&#x7684;&#x8868;&#x73B0;&#xFF01;&#x56E0;&#x4E3A;&#x5B83;&#x4F1A;&#x9650;&#x5236;&#x7F51;&#x7EDC;&#x8FC7;&#x5EA6;&#x62DF;&#x5408;&#x8BAD;&#x7EC3;&#x96C6;&#x7684;&#x80FD;&#x529B;. &#x4F46;&#x662F;&#x5B83;&#x6700;&#x7EC8;&#x4F1A;&#x63D0;&#x9AD8;&#x6D4B;&#x8BD5;&#x96C6;&#x7684;&#x8868;&#x73B0;.</p>
<p><strong>&#x8BB0;&#x4F4F;</strong>:</p>
<ul>
<li>&#x6B63;&#x5219;&#x5316;&#x80FD;&#x5E2E;&#x52A9;&#x4F60;&#x51CF;&#x5C11;&#x8FC7;&#x62DF;&#x5408;&#x3002;</li>
<li>&#x6B63;&#x5219;&#x5316;&#x4F1A;&#x63A8;&#x52A8;&#x4F60;&#x7684;&#x6743;&#x91CD;&#x5411;&#x66F4;&#x5C0F;&#x7684;&#x503C;&#x8F6C;&#x53D8;.</li>
<li>L2 regularization &#x548C; Dropout &#x662F;&#x4E24;&#x79CD;&#x975E;&#x5E38;&#x6709;&#x6548;&#x7684;&#x6B63;&#x5219;&#x5316;&#x6280;&#x672F;.
<ul>
<li>Dropout &#x66F4;&#x6539;&#x524D;&#x5411;&#x4F20;&#x64AD;&#x51FD;&#x6570;&#x548C;&#x540E;&#x5411;&#x4F20;&#x64AD;&#x51FD;&#x6570;</li>
<li>L2 &#x4FEE;&#x6539;&#x635F;&#x5931;&#x51FD;&#x6570;&#x4EE5;&#x53CA;&#x7531;&#x635F;&#x5931;&#x51FD;&#x6570;&#x8BA1;&#x7B97;&#x7684;&#x68AF;&#x5EA6;&#xFF08;&#x540E;&#x5411;&#x4F20;&#x64AD;&#x51FD;&#x6570;&#xFF09;</li>
</ul>
</li>
</ul>

      </div>
      
      
    
    
    
    
    
    
    
    
  
    </body></html>