+++
author = "Angyi"
comments = true
date= 2019-07-16T20:27:12+08:00
draft = false
tags=["Machine-Learning"]
Menu = "ml"
title = "1.2.1-4：神经网络的编程基础"
Summary = " 【Basics of Neural Network programming】第一门课第二周1-4小节；   1. 基本案例，辨识猫，二分类模型      2.  逻辑回归     3. 损失函数      4. 梯度下降法 "

+++

Basics of Neural Network programming

### 2.1 二分类(Binary Classification)

这周我们将学习神经网络的基础知识，其中需要注意的是，当实现一个神经网络的时候，我们需要知道一些非常重要的技术和技巧。例如有一个包含$m$个样本的训练集，你很可能习惯于用一个`for`循环来遍历训练集中的每个样本，但是当实现一个神经网络的时候，我们通常不直接使用`for`循环来遍历整个训练集，所以在这周的课程中你将学会如何处理训练集。

另外在神经网络的计算中，通常先有

1. 一个叫做前向暂停(`forward pause`)或叫做前向传播(`foward propagation`)的步骤

2. 接着有一个叫做反向暂停(`backward pause`) 或叫做反向传播`(backward propagation`)的步骤。

所以这周我也会向你介绍为什么神经网络的训练过程可以分为前向传播和反向传播两个独立的部分。

在课程中我将使用***\*逻辑回归\****(`logistic regression`)来传达这些想法，以使大家能够更加容易地理解这些概念。即使你之前了解过逻辑回归，我认为这里还是有些新的、有趣的东西等着你去发现和了解，所以现在开始进入正题。

逻辑回归是一个用于***\*二分类\****(**binary classification**)的算法。首先我们从一个问题开始说起，这里有一个二分类问题的例子，假如你有一张图片作为输入，比如这只猫，如果识别这张图片为猫，则输出标签1作为结果；如果识别出不是猫，那么输出标签0作为结果。现在我们可以用字母 $y$来 表示输出的结果标签，如下图所示：

![269118812ea785aee00f6ffc11b5c882](https://i.imgur.com/oKcGAZc.png)

我们来看看一张图片在计算机中是如何表示的，为了保存一张图片，需要保存三个矩阵，它们分别对应图片中的红、绿、蓝三种颜色通道，如果你的图片大小为64x64像素，那么你就有三个规模为64x64的矩阵，分别对应图片中红、绿、蓝三种像素的强度值。为了便于表示，这里我画了三个很小的矩阵，注意它们的规模为5x4 而不是64x64，如下图所示：

![1e664a86fa2014d5212bcb88f1c419cf](https://i.imgur.com/JY61IiQ.png)

\> 为了把这些像素值放到一个特征向量中，我们需要把这些像素值提取出来，然后放入一个特征向量$x$。

为了把这些像素值转换为特征向量 $x$，我们需要像下面这样定义一个特征向量 $x$ 来表示这张图片，我们把所有的像素都取出来，例如255、231等等，直到取完所有的红色像素，接着最后是255、134、…、255、134等等，直到得到一个特征向量，把图片中所有的红、绿、蓝像素值都列出来。

**如果图片的大小为64x64像素，那么向量 $x$ 的总维度，将是64乘以64乘以3，这是三个像素矩阵中像素的总量。在这个例子中结果为12,288。现在我们用$n_x=12,288$，来表示输入特征向量的维度，有时候为了简洁，我会直接用小写的$n$来表示输入特征向量$x$的维度。**

所以在二分类问题中，我们的目标就是习得一个分类器，它以图片的特征向量作为输入，然后预测输出结果$y$为1还是0，也就是预测图片中是否有猫：

![e173fd42de5f1953deb617623d5087e8](https://i.imgur.com/L2bfi4F.png)

接下来我们说明一些在余下课程中，需要用到的一些符号。

‘符号定义‘ ：

$x$：表示一个$n_x$维数据，为输入数据，维度为$(n_x,1)$； 

$y$：表示输出结果，取值为$(0,1)$；

$(x^{(i)},y^{(i)})$：表示第$i$组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据； 

$X=[x^{(1)},x^{(2)},...,x^{(m)}]$：表示所有的训练数据集的输入值，放在一个 $n_x×m$的矩阵中，其中$m$表示样本数目; 

$Y=[y^{(1)},y^{(2)},...,y^{(m)}]$：对应表示所有训练数据集的输出值，维度为$1×m$。

用一对$(x,y)$来表示一个单独的样本，$x$代表$n_x$维的特征向量，$y$ 表示标签(输出结果)只能为0或1。

而训练集将由$m$个训练样本组成，其中$(x^{(1)},y^{(1)})$表示第一个样本的输入和输出，$(x^{(2)},y^{(2)})$表示第二个样本的输入和输出，直到最后一个样本$(x^{(m)},y^{(m)})$，然后所有的这些一起表示整个训练集。有时候为了强调这是训练样本的个数，会写作$M_{train}$，当涉及到测试集的时候，我们会使用$M_{test}$来表示测试集的样本数，所以这是测试集的样本数：

![12f602ed40ba90540112ae0fee77fadf](https://i.imgur.com/yLPUgTN.png)

最后为了能把训练集表示得更紧凑一点，我们会定义一个矩阵用大写$X$的表示，它由输入向量$x^{(1)}$、$x^{(2)}$等组成，如下图放在矩阵的列中，所以现在我们把$x^{(1)}$作为第一列放在矩阵中，$x^{(2)}$作为第二列，$x^{(m)}$放到第$m$列，然后我们就得到了训练集矩阵$X$。所以这个矩阵有$m$列，$m$是训练集的样本数量，然后这个矩阵的高度记为$n_x$，注意有时候可能因为其他某些原因，矩阵$X$会由训练样本按照行堆叠起来而不是列，如下图所示：$x^{(1)}$的转置直到$x^{(m)}$的转置，但是在实现神经网络的时候，使用***\*左边的这种形式\****，会让整个实现的过程变得更加简单：

![55345ba411053da11ff843bbb3406369](https://i.imgur.com/CXRWsQl.png)

现在来简单温习一下:$X$是一个规模为$ n_x $乘以$m$的矩阵，当你用`Python`实现的时候，你会看到`X.shape`，这是一条`Python`命令，用于显示矩阵的规模，即`X.shape`等于$(n_x,m)$，$X$是一个规模为$n_x$乘以$m$的矩阵。所以综上所述，这就是如何将训练样本（输入向量$X$的集合）表示为一个矩阵。

那么输出 **标签y**呢？同样的道理，为了能更加容易地实现一个神经网络，将标签$y$放在列中将会使得后续计算非常方便，所以我们定义大写的$Y$等于${{y}^{\left( 1 \right)}},{{y}^{\left( m \right)}},...,{{y}^{\left( m \right)}}$，所以在这里是一个规模为1乘以$m$的矩阵，同样地使用`Python`将表示为`Y.shape`等于$(1,m)$，表示  ***\*这是一个规模为1乘以$m$的矩阵。\****

![55345ba411053da11ff843bbb3406369](https://i.imgur.com/CXRWsQl.png)

当你在后面的课程中实现神经网络的时候，你会发现，一个好的符号约定能够将不同训练样本的数据很好地组织起来。而我所说的数据不仅包括 $x$ 或者 $y$ 还包括之后你会看到的其他的量。将不同的训练样本的数据提取出来，然后就像刚刚我们对 $x$ 或者 $y$ 所做的那样，将他们堆叠在矩阵的列中，形成我们之后会在逻辑回归和神经网络上要用到的符号表示。如果有时候你忘了这些符号的意思，比如什么是 $m$，或者什么是 $n$，或者忘了其他一些东西，我们也会在课程的网站上放上符号说明，然后你可以快速地查阅每个具体的符号代表什么意思，好了，我们接着到下一个视频，在下个视频中，我们将以逻辑回归作为开始。

备注：附录里也写了符号说明。

**### 2.2 逻辑回归(Logistic Regression)**

在这个视频中，我们会重温逻辑回归学习算法，该算法适用于二分类问题，本节将主要介绍逻辑回归的`Hypothesis Function`（假设函数）。

对于二元分类问题来讲，给定一个输入特征向量$X$，它可能对应一张图片，你想识别这张图片识别看它是否是一只猫或者不是一只猫的图片，你想要一个算法能够输出预测，你只能称之为$\hat{y}$，也就是你对实际值 $y$ 的估计。更正式地来说，你想让 $\hat{y}$ 表示 $y$ 等于1的一种可能性或者是机会，前提条件是给定了输入特征$X$。换句话来说，如果$X$是我们在上个视频看到的图片，你想让 $\hat{y}$ 来告诉你这是一只猫的图片的机率有多大。在之前的视频中所说的，$X$是一个$n_x$维的向量（相当于有$n_x$个特征的特征向量）。我们用$w$来表示逻辑回归的参数，这也是一个$n_x$维向量（因为$w$实际上是特征权重，维度与特征向量相同），参数里面还有$b$，这是一个实数（表示偏差）。所以给出输入$x$以及参数$w$和$b$之后，我们怎样产生输出预测值$\hat{y}$，一件你可以尝试却不可行的事是让$\hat{y}={{w}^{T}}x+b$。

![dfb5731c30b81eced917450d31e860a3](https://i.imgur.com/S0XDe3S.png)

这时候我们得到的是一个关于输入$x$的线性函数，实际上这是你在做线性回归时所用到的，但是这对于二元分类问题来讲不是一个非常好的算法，因为你想让$\hat{y}$表示实际值$y$等于1的机率的话，$\hat{y}$ 应该在0到1之间。这是一个需要解决的问题，因为${{w}^{T}}x+b$可能比1要大得多，或者甚至为一个负值。对于你想要的在0和1之间的概率来说它是没有意义的，因此在逻辑回归中，我们的输出应该是$\hat{y}$等于由上面得到的线性函数式子作为自变量的`sigmoid`函数中，公式如上图最下面所示，将线性函数转换为非线性函数。

下图是`sigmoid`函数的图像，如果我把水平轴作为$z$轴，那么关于$z$的`sigmoid`函数是这样的，它是平滑地从0走向1，让我在这里标记纵轴，这是0，曲线与纵轴相交的截距是0.5，这就是关于$z$的`sigmoid`函数的图像。我们通常都使用$z$来表示${{w}^{T}}x+b$的值。

![](../images/7e304debcca5945a3443d56bcbdd2964.png)

关于`sigmoid`函数的公式是这样的，$\sigma \left( z \right)=\frac{1}{1+{{e}^{-z}}}$,在这里$z$是一个实数，这里要说明一些要注意的事情，如果$z$非常大那么${{e}^{-z}}$将会接近于0，关于$z$的`sigmoid`函数将会近似等于1除以1加上某个非常接近于0的项，因为$e$ 的指数如果是个绝对值很大的负数的话，这项将会接近于0，所以如果$z$很大的话那么关于$z$的`sigmoid`函数会非常接近1。相反地，如果$z$非常小或者说是一个绝对值很大的负数，那么关于${{e}^{-z}}$这项会变成一个很大的数，你可以认为这是1除以1加上一个非常非常大的数，所以这个就接近于0。实际上你看到当$z$变成一个绝对值很大的负数，关于$z$的`sigmoid`函数就会非常接近于0，因此当你实现逻辑回归时，你的工作就是去让机器学习参数$w$以及$b$这样才使得$\hat{y}$成为对$y=1$这一情况的概率的一个很好的估计。

![](../images/f5049dc7ce815b495fbbdf71f23fc66c.png)

在继续进行下一步之前，介绍一种***\*符号惯例\****，可以让参数$w$和参数$b$分开。在符号上要注意的一点是当我们对神经网络进行编程时经常会让参数$w$和参数$b$分开，在这里参数$b$对应的是一种偏置。在之前的机器学习课程里，你可能已经见过处理这个问题时的其他符号表示。比如在某些例子里，你定义一个额外的特征称之为${{x}_{0}}$，并且使它等于1，那么现在$X$就是一个$n_x$加1维的变量，然后你定义$\hat{y}=\sigma \left( {{\theta }^{T}}x \right)$的`sigmoid`函数。在这个备选的符号惯例里，你有一个参数向量${{\theta }*_{0}},{{\theta }_*{1}},{{\theta }*_{2}},...,{{\theta }_*{{{n}*_{x}}}}$，这样${{\theta }_*{0}}$就充当了$b$，这是一个实数，而剩下的${{\theta }*_{1}}$ 直到${{\theta }_*{{{n}_{x}}}}$充当了$w$，结果就是当你实现你的神经网络时，有一个比较简单的方法是保持$b$和$w$分开。但是在这节课里我们不会使用任何这类符号惯例，所以不用去担心。

现在你已经知道逻辑回归模型是什么样子了，下一步要做的是训练参数$w$和参数$b$，你需要定义一个代价函数，让我们在下节课里对其进行解释。

### 2.3 逻辑回归的代价函数（Logistic Regression Cost Function）**

在上个视频中，我们讲了逻辑回归模型，这个视频里，我们讲逻辑回归的代价函数（也翻译作成本函数）。

#### 为什么需要代价函数：

为了训练逻辑回归模型的参数参数$w$和参数$b$我们，需要一个代价函数，通过训练代价函数来得到参数$w$和参数$b$。先看一下逻辑回归的输出函数：

![4c9a27b071ce9162dbbcdad3393061d2](https://i.imgur.com/X1H46dV.png)

为了让模型通过学习调整参数，你需要给予一个$m$样本的训练集，这会让你在训练集上找到参数$w$和参数$b$,，来得到你的输出。

对训练集的预测值，我们将它写成$\hat{y}$，我们更希望它会接近于训练集中的$y$值，为了对上面的公式更详细的介绍，我们需要说明上面的定义是对一个训练样本来说的，这种形式也使用于每个训练样本，我们使用这些带有圆括号的上标来区分索引和样本，训练样本$i$所对应的预测值是${{y}^{(i)}}$,是用训练样本的${{w}^{T}}{{x}^{(i)}}+b$然后通过`sigmoid`函数来得到，也可以把$z$定义为${{z}^{(i)}}={{w}^{T}}{{x}^{(i)}}+b$,我们将使用这个符号$(i)$注解，上标$(i)$来指明数据表示$x$或者$y$或者$z$或者其他数据的第$i$个训练样本，这就是上标$(i)$的含义。

**损失函数：**

损失函数又叫做误差函数，用来衡量算法的运行情况，`Loss function:`$L\left( \hat{y},y \right)$.

我们通过这个$L$称为的损失函数，来衡量预测输出值和实际值有多接近。一般我们用预测值和实际值的平方差或者它们平方差的一半，但是通常在逻辑回归中我们不这么做，因为当我们在学习逻辑回归参数的时候，会发现我们的优化目标不是凸优化，只能找到多个局部最优值，梯度下降法很可能找不到全局最优值，虽然平方差是一个不错的损失函数，但是我们在逻辑回归模型中会定义另外一个损失函数。

我们在逻辑回归中用到的损失函数是：$L\left( \hat{y},y \right)=-y\log(\hat{y})-(1-y)\log (1-\hat{y})$

为什么要用这个函数作为逻辑损失函数？当我们使用平方误差作为损失函数的时候，你会想要让这个误差尽可能地小，对于这个逻辑回归损失函数，我们也想让它尽可能地小，为了更好地理解这个损失函数怎么起作用，我们举两个例子：

当$y=1$时损失函数$L=-\log (\hat{y})$，如果想要损失函数$L$尽可能得小，那么$\hat{y}$就要尽可能大，因为`sigmoid`函数取值$[0,1]$，所以$\hat{y}$会无限接近于1。

当$y=0$时损失函数$L=-\log (1-\hat{y})$，如果想要损失函数$L$尽可能得小，那么$\hat{y}$就要尽可能小，因为`sigmoid`函数取值$[0,1]$，所以$\hat{y}$会无限接近于0。

在这门课中有很多的函数效果和现在这个类似，就是如果$y$等于1，我们就尽可能让$\hat{y}$变大，如果$y$等于0，我们就尽可能让 $\hat{y}$ 变小。

损失函数是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现如何，我们需要定义一个算法的代价函数，算法的代价函数是对$m$个样本的损失函数求和然后除以$m$:

$J\left( w,b \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{L\left( {{{\hat{y}}}^{(i)}},{{y}^{(i)}} \right)}=\frac{1}{m}\sum\limits_{i=1}^{m}{\left( -{{y}^{(i)}}\log {{{\hat{y}}}^{(i)}}-(1-{{y}^{(i)}})\log (1-{{{\hat{y}}}^{(i)}}) \right)}$

损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价，所以在训练逻辑回归模型时候，我们需要找到合适的$w$和$b$，来让代价函数 $J$ 的总代价降到最低。

根据我们对逻辑回归算法的推导及对单个样本的损失函数的推导和针对算法所选用参数的总代价函数的推导，结果表明逻辑回归可以看做是一个非常小的神经网络，在下一个视频中，我们会看到神经网络会做什么。

### 2.4 梯度下降法（Gradient Descent）

**梯度下降法可以做什么？**

在你测试集上，通过最小化代价函数（成本函数）$J(w,b)$来训练的参数$w$和$b$，

![cbd5ff8c461fcb5a699c4ec4789687b3](https://i.imgur.com/nkdTgU0.jpg)

如图，在第二行给出和之前一样的逻辑回归算法的代价函数（成本函数）

\> 梯度下降法的形象化说明

![a3c81d2c8629d674141def47dc02f312](https://i.imgur.com/42dFrSP.jpg)

在这个图中，横轴表示你的空间参数$w$和$b$，在实践中，$w$可以是更高的维度，但是为了更好地绘图，我们定义$w$和$b$，都是单一实数，代价函数（成本函数）$J(w,b)$是在水平轴$w$和$b$上的曲面，因此曲面的高度就是$J(w,b)$在某一点的函数值。我们所做的就是找到使得代价函数（成本函数）$J(w,b)$函数值是最小值，对应的参数$w$和$b$。

![236774be30d12524a2002c3c484d22d5](https://i.imgur.com/ySxM0Da.jpg)

如图，代价函数（成本函数）$J(w,b)$是一个凸函数(`convex function`)，像一个大碗一样。

![af11ecd5d72c85f777592f8660678ce6](https://i.imgur.com/UfYXUon.jpg)

如图，这就与刚才的图有些相反，因为它是非凸的并且有很多不同的局部最小值。由于逻辑回归的代价函数（成本函数）$J(w,b)$特性，我们必须定义代价函数（成本函数）$J(w,b)$为凸函数。

1. 初始化$w$和$b$，

可以用如图那个小红点来初始化参数$w$和$b$，也可以采用随机初始化的方法，对于逻辑回归几乎所有的初始化方法都有效，因为函数是凸函数，无论在哪里初始化，应该达到同一点或大致相同的点。

![0ad6c298d0ac25ca9b26546bb06d462c](https://i.imgur.com/MaD0mMb.jpg)

我们以如图的小红点的坐标来初始化参数$w$和$b$。

2. 朝最陡的下坡方向走一步，不断地迭代

![bb909b874b2865e66eaf9a5d18cc00e5](https://i.imgur.com/RBEcODz.jpg)

我们朝最陡的下坡方向走一步，如图，走到了如图中第二个小红点处。

![c5eda5608fd2f4d846559ed8e89ed33c](https://i.imgur.com/YXR6ELi.jpg)

我们可能停在这里也有可能继续朝最陡的下坡方向再走一步，如图，经过两次迭代走到第三个小红点处。

3. 直到走到全局最优解或者接近全局最优解的地方

通过以上的三个步骤我们可以找到全局最优解，也就是代价函数（成本函数）$J(w,b)$这个凸函数的最小值点。

> 梯度下降法的细节化说明（仅有一个参数）

![5300d40870ec58cb0b8162747b9559b9](https://i.imgur.com/RFQGDZb.jpg)

假定代价函数（成本函数）$J(w)$ 只有一个参数$w$，即用一维曲线代替多维曲线，这样可以更好画出图像。

![6cdef1989a113fc1caaaaf6ebaaa3549](https://i.imgur.com/9nnqZs5.jpg)

![60cc674531ac72b2d75b0c447db95e96](https://i.imgur.com/HfxLBsz.jpg)

迭代就是不断重复做如图的公式:

$:=$表示更新参数,

$a $ 表示学习率（`learning rate`），用来控制步长（`step`），即向下走一步的长度$\frac{dJ(w)}{dw}$  就是函数$J(w)$对$w$ 求导（`derivative`），在代码中我们会使用$dw$表示这个结果
![27be50001e7a91bd2abaaeaf7aba7cd4](https://i.imgur.com/oG2EHJx.jpg)


对于导数更加形象化的理解就是斜率（`slope`），如图该点的导数就是这个点相切于 $J(w)$的小三角形的高除宽。假设我们以如图点为初始化点，该点处的斜率的符号是正的，即$\frac{dJ(w)}{dw}>0$，所以接下来会向左走一步。
![4fb3b91114ecb2cd81ec9f3662434d81](https://i.imgur.com/s7M7dCf.jpg)


整个梯度下降法的迭代过程就是不断地向左走，直至逼近最小值点。
![579fb3957063480420c6a7d294503e97](https://i.imgur.com/hOqXsk2.jpg)


假设我们以如图点为初始化点，该点处的斜率的符号是负的，即$\frac{dJ(w)}{dw}<0$，所以接下来会向右走一步。
![21541fc771ad8895c18d292dd4734fe7](https://i.imgur.com/yQpbPiw.jpg)


整个梯度下降法的迭代过程就是不断地向右走，即朝着最小值点方向走。

> 梯度下降法的细节化说明（两个参数）

逻辑回归的代价函数（成本函数）$J(w,b)$是含有两个参数的。

![593eb7e2835b4f3c3aa8185cfa76155c](https://i.imgur.com/i5Dv4b4.png)

$\partial $ 表示求偏导符号，可以读作`round`，

$\frac{\partial J(w,b)}{\partial w}$  就是函数$J(w,b)$ 对$w$ 求偏导，在代码中我们会使用$dw$ 表示这个结果，

$\frac{\partial J(w,b)}{\partial b}$  就是函数$J(w,b)$对$b$ 求偏导，在代码中我们会使用$db$ 表示这个结果，

小写字母$d$ 用在求导数（`derivative`），即函数只有一个参数，

偏导数符号$\partial $ 用在求偏导（`partial derivative`），即函数含有两个以上的参数。